---
layout:     post
title:      Intrinsically Motivated Multi-Task Multi-Goal RL
date:       2019-04-09 11:21:29
summary:    Curious can target multiple multi-goal task using a single policy. It is intrinsically motivated to choose its own tasks and goals. It tracks its own competence and competence progress and focuses on tasks with high progress. This enables efficient learning, resistance to distracting tasks, forgetting and sensory failures. 
categories: jekyll
permalink: curious_intrinsically_motivated_multi_task_multi_goal_rl
use_math: true
---



<div align="center">
<iframe width="70%" height="300" src="https://www.youtube.com/embed/F8KQu8eNhcg" frameborder="0" allowfullscreen></iframe>
</div>



## Introduction

In Reinforcement Learning (RL), agents are usually provided a unique task, well-defined by an associated reward function that provides positive feedbacks when the task is fullfilled, negative feedbacks otherwise. If a domestic robot sets the table, it is rewarded, if the plates are on the floor, it is not. The objective of that agent is to maximize the sum of collected rewards.

In the more realistic open-ended and changing environments, agents face a wide range of potential tasks that might not come with associated reward functions. Such autonomous learning agents must set their own tasks and build their own curriculum through an intrinsically motivated exploration. They must decide for themselves what to practice and what to learn. Because some tasks might prove easy and some impossible, agents must actively select which task to practice at any given moment, to maximize their overall mastery on the set of learnable tasks. 

This blog post presents CURIOUS, an algorithm rooted in developmental robotics and builds on two main fields:

* **Multi-goal RL.** Agents traditionally learn to perform one well-defined task. On the contrary, Multi-Goal RL trains agents on a goal-parameterized task. Instead of learning to bring the TV remote at this special spot on the table, we can now train it to bring it at any particular location (goal), in the living room, on the sofa etc. Learning about a precise goal benefits learning about others as well, which speeds up learning.

* **Curriculum Learning.** When facing different possible tasks (e.g. going to the kitchen, fetching the remote, cleaning the floor), the agent needs to prioritize and decide which task to practice at any given moment. Developmental Robotics presents mechanism to help this task arbitration. Optimizing for learning progress for example, enables an automatic curriculum to emerge. First, train on simple tasks. When they are mastered, move on to others where progress is made.


All details can be found in the [paper](https://arxiv.org/abs/1810.06284). The [algorithm](https://github.com/flowersteam/curious) and the [environment](https://github.com/flowersteam/gym_flowers) can be found on Github. 



## The Problem of Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning

<div align="center">
<img class="80" src="https://openlab-flowers.inria.fr/uploads/default/original/2X/1/15ea4a22bd3ebbe32ad0b9afddd36b9647563c34.png" width="80%" alt="The Multi-Task, Multi-Goal Fetch Arm environment." />
<div>
<sub>
<i><b>Multi-Task, Multi-Goal Fetch Arm</b>: an environment with multiple potential tasks with various levels of difficulty, from simple to impossible (Reach, Push, Pick and Place, Stack, Push out-of-reach cube). Each task is parameterized by a goal (target).</i></sub>
</div>
</div>

Agents in the real world might face a large number of potential _tasks_. A domestic robot might want to clean up a table, to prepare the meal, to set the table etc. In addition, some tasks might be parameterized by _goals_ or _targets_. If "move the plates" can be seen as a task, the _goal_ indicates where they should be moved (e.g. to the cupboard). Tasks can be more generally defined as constraints on the state or trajectory of states. "Move the plate" requires a modification of the position of these plates, the goal is the parameter specifying _where_. 

This multi-task, multi-goal setting is simulated in our Multi-Task Multi-Goal Fetch Arm environment. Adapted from [OpenAI Gym](https://github.com/openai/gym)'s Fetch Arm environments, the robotic arm faces a table and several cubes, and can decide to _Reach_ a 3D target (goal) with its gripper, to _Push_ a cube on a 2D target, to _Pick and Place_ a cube on a 3D target or to _Stack_ one cube on top of another. Several out-of-reach cubes are added to the scene to represent _distacting tasks_: tasks that are impossible to solve by the agent. These cubes are moving randomly and perceived by the agent.

This problem is seen through the lens of the [Intrinsically Motivated Goal Exploration Process](https://arxiv.org/abs/1708.02190) (IMGEP)  framework. The agent decides itself which task and goal to target, which task and goal to train on at any given moment. It is intrinsically motivated to set its own goals to explore its surroundings, with the objective of mastering all tasks that can be mastered. The number of potential tasks might be large, some tasks might be easy, others difficult or even impossible. This advocates for curriculum learning mechanisms to enable efficient experience collection and training.

## Previous Work

As mentionned above, CURIOUS integrates and extends two lines of research: Multi-Goal RL and Curriculum Learning.

The state-of-the-art Multi-Goal RL architecture is [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf) (UVFA). It proposes to condition the  policy (controller) and the value function (predictor of future rewards) by the current goal in a multi-goal setting. This enables to target goals drawn from a continuous space (e.g. target maze location, target gripper position) and efficient generalization across goals. [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495) (HER) proposed to generate imagined goals to learn about, when a trajectory did not achieve its original goal (counterfactual learning, see figure below). [UNICORN](https://arxiv.org/abs/1802.08294) introduced a task-conditioned policy to target a set of discrete tasks and used discrete counterfactual learning (replacing the original task by a random imagined task from the task-set). All these algorithms are based on UVFA and the idea of having a controller that uses the goal as input. Although the term _goal_ is defined quite generally in the paper, previous research has mostly used simple goal representations. In the original UVFA paper, a goal is a target position in a maze, in HER it is a 3D target position for the gripper, in UNICORN it is the type of object to reach. Furthermore, the multi-goal RL community has focused on goal defined externally, provided by the experimenter for the agent to execute.


<div align="center">
<img class="80" src="/images/posts/curious/her.png" width="80%" alt="Counterfactual learning." />
<div>
<sub>
<i><b>Counterfactual Learning with HER</b>. From <a href="https://openai.com/blog/ingredients-for-robotics-research/"> OpenAI blog </a>.</i></sub>
</div>
</div>

CURIOUS builds on the developmental robotics research and considers the agents to be empowered to select their own tasks and goals. We use previously defined mechanisms for autonomous curriculum generation. As in [MACOB](https://hal.archives-ouvertes.fr/hal-01384566/document) and the [IMGEP](https://arxiv.org/abs/1708.02190) framework, CURIOUS tracks its competence and learning progress on each task and maximizes learning progress based on a multi-armed bandit algorithm. The use of learning progress was previously used in combination with memory-based learning algorithms. For each episode, the agent stores a pair made of a controller and a description of the outcome of the episode. This type of algorithm is hard to scale because of memory issues and is generally quite sensitive to the distribution of initial conditions.

The CURIOUS agent extends these two lines of work with two main contributions. First, it enables to target multiple multi-goal tasks in a unique controller by proposing a new encoding. The policy is therefore conditionned by both the current task and the current goal, enabling efficient generalisation across multiple tasks and goals. Second, we use mechanisms based on learning progress in combination with an RL algorithm. In addition to using learning progress to select the next task to target, we also use learning progress to decide which task to train on.

## A Modular Goal Encoding: E-UVFA

The most intuitive way to target multiple multi-goal tasks would be to use a multi-goal policy for each task. We call this architecture _Multi-Goal Task Expert_ (MG-TE). With CURIOUS, we propose the _Extended-UVFA_ encoding to target multiple multi-goal tasks in a single policy. The input of the policy (and value function) is now the concatenation of the current state, a one-hot encoding of the task and a goal vector. The goal vector is the concatenation of the goals in each task, where the goals of unconsidered tasks are set to $0$. In the figure, the agent targets task $T_1$ $(t_d=[1, 0])$ out of $2$ tasks and targets the 2D goal $g_1 = [g_{11},g_{12}]$ for task $T_1$, e.g. Pushing the yellow cube at position $g_1$ on the table. The underlying learning algorithm is [Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971) (DDPG). We use discrete counterfactual learning for cross-task learning and HER for counterfactual goal learning. This consists in replacing the original task descriptor and goal in the transition by others. HER replaces the original goal by an outcome achieved later in the trajectory. UNICORN replaces the original task by a random task from the task-set. In other word, our agent can use any past experience to train on any task and goal by pretending it was targeting them originally.




<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/5/5d3296b49ab9f1909dcefe97795c7af5debc0460.png" width="70%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Actor-Critic networks using the E-UVFA architecture</b>: In green a discrete one-hot encoding of the current task. In yellow the goal vector, concatenation of the goal vectors (targets) of each task. When a task is selected, only the sub-vector corresponding to that task is activated. </i></sub>
</div>
</div>


The figure below demonstrates the advantage of using a unique policy and value function to target all tasks and goals at once. We run $10$ trials for each architecture on a set of $4$ tasks and report the average success rate over the four tasks. As a sanity check demonstrating the need to use a modular representation of tasks, we try the HER algorithm, where goals are drawn from a flat representation (e.g. put the cube at position $x_1$, while reaching position $x_2$ with the gripper). As almost none of these goals can be reached in practice, the performance of HER stays null.


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/6/668443d3bbea2c4a5627fba9e3e5b6e2db4dbabd.png"  width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Impact of the policy and value function architecture.</b> Average success rates computed over achievable tasks. Mean +/- standard deviation over 10 trials are plotted, while dots indicate significance when testing E-UVFA against MG-TE with a Welch's t-test. </i></sub>
</div>
</div>



## Automatic Curriculum with Learning Progress

<div align="center">
<img class="80" src="/images/posts/curious/lp.png" width="100%" alt="Counterfactual learning." />
<div>
<sub>
<i><b>Computing competence, learning progress, and task probabilities.</b>. The agent keeps track of past successes and failures using a limited_size history per task ($N=6$ here)(top). Using these histories, it can compute its own competence on each task using the success rate over the last 6 attempts (left). It can also track its learning progress as the difference between success rates computed over the last 3 attempts and the previous 3 attempts. Finally, the agent computes selection probabilities based on these measures (right).</i></sub>
</div>
</div>

Our agent tracks its competence and learning progress (LP) on each task. To do that, it performs self-evaluation episodes without exploration noise, and records for each task the list of past successes and failures. The competence in a task is simply the success rate over the recent history. The learning progress is defined as the derivative of the competence, and is empirically computed using a difference of success rates computed over two consecutive and non-overlapping windows from the recent history. The figure below presents an example of these self-evaluations.

The learning progress measures are used for two purposes:
* To select which task to target next (as in MACOB).
* To select which task to train on (new).

The problem of tasks selection can be seen as a non-stationary multi-armed bandit problem, where the value to maximize is learning progress. We compute selection probabilities using an epsilon-greedy proportion rule based on the absolute measures of learning progress:

$$
p(T_i) = \frac{\epsilon}{N} + (1-\epsilon) \frac{\mid LP(T_i)\mid}{\sum_j \mid LP(T_j)\mid},
$$

where $N$ is the number of tasks, $LP(T_i)$ is the learning progress computed on task $T_i$. 




These probabilities are used to select the next task to target, and to bias the counterfactual learning of tasks. Substituting the original task by another enables to focus learning on the substitute task. When the agent thinks about that time it was trying to lift the glass but tries to pretend it was pushing the glass, it learns about pushing the glass. If the agent tries to think about many experiences with the imagined goal of pushing the glass, it might learn how to do it. It might even learn that task without   having ever targeted it before! Using LP measures enables the agent to control on which task to focus its learning. It first focuses on simple tasks where it is making progress. When they are mastered, they become less interesting and the agent focuses on new tasks. Following the learning progress automatically builds a curriculum learning strategy.

The figure below shows the competence, learning progress and selection probabilities computed internally by the agent over the whole run. It is like having access to the inner variables it uses to make decisions. We interpret these curves as a developmental trajectory of the agent. First, it learns how to control its gripper ($T_1$, blue). When it knows how to, learning progress drops, making this task less interesting. It then focuses on another task where it has started to make progress (pushing the cube, orange). Finally, it learns to pick and place and stack cubes (green and yellow respectively). 

Around $75.10^3$ episodes, the agent detects a drop in its competence in the Pick and Place task, this triggers an increase of the absolute progress which ultimately results in a renewed focus on that task, enabling to mitigate the performance drop. Using the absolute value of learning progress helps to resist forgetting.


<div align="center">
<table>
<tr>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/0/0e627db6b4126c779880462d21ef39ecdf0f35a2.png" height="150"   />
</td>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/e/e1ef389a8fa7aa109e9a2561f0aec81881fcff95.png" height="150"   />
</td>
<td>
<img class='special' src="https://openlab-flowers.inria.fr/uploads/default/original/2X/b/b799cd450ecfd4fcea465b76ccc6ca28ac498632.png" height="150"  />
</td>
</tr>
</table>
</div>
<sub>
<i><b>Competence, learning progress and developmental trajectories</b>: Left: competence for each task in one run of the algorithm. Middle:  corresponding absolute learning progress. Right: corresponding task probabilities.</i></sub>




## Resilience to Distracting Tasks

In the real world, not all tasks can be achieved. We simulate this with extra tasks where the agent needs to push out-of-reach cubes on 2D locations. As these tasks are impossible, the learning progress measure stays flat, which enables the agent to focus on more relevant tasks. When the number of distracting tasks increases $(0,4,7)$ in addition to the set of four tasks described earlier, the use of the learning progress task selection and replay (CURIOUS) improves over the random task selection and replay (E-UVFA only).


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/7/73e801d28a024ea602c765a97abea092e5e3e6df.png" width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Resilience to distracting tasks</b>: Different colors represent different number of distracting tasks (Pushing an out-o-reach cube). There are four achievable tasks. Dots indicate significant differences between CURIOUS (intrinsically motivated) and E-UVFA (random task), using a Welch's t-test and 10 seeds. Mean and standard error of the mean plotted. </i></sub>
</div>
</div>



## Resilience to Forgetting and Sensory Failures


Using absolute learning progress measures enables the agent to detect drops in performance. Here, we simulate a time-locked sensory failure: the sensor reporting the position of one of the cube is shifted by the size of a cube. The performance on the Push task related to that cube (one of the four tasks) suddenly drops, making the average success rate over all tasks drop by a quarter (see figure below). We then compare E-UVFA (random task selection and replay) and CURIOUS (using LP) during the recovery. CURIOUS manages to recover $95\%$ of its pre-perturbation performance $45\%$ faster than its random counterpart.


<div align="center">
<img src="https://openlab-flowers.inria.fr/uploads/default/original/2X/3/3af006ba74e359c9970c99cc6c339e08872faad7.png" width="80%" alt="The E-UVFA architecture" />
<div>
<sub>
<i><b>Resilience to sensory failure</b>: Recovery following a sensory failure. CURIOUS recovers 90% of its original performance twice as fast as E-UVFA. Dots indicate significant differences in mean performance (Welch's t-test, 10 random seeds). Mean and standard deviations are reported.</i></sub>
</div>
</div>


## Discussion

As noted in [Mankowitz et al., 2018](https://arxiv.org/abs/1802.08294), representations of the world state are learned in the first layers of a neural network policy/value function. Sharing these representations across all tasks and goals explains the important difference between the E-UVFA encoding and the use of multiple task-expert policies. However, learning all tasks in the same policy might become difficult as the number of tasks increases, and when tasks are different from one another (e.g. using different sensory modalities). Catastrophic forgetting can also play a role, as previously mastered tasks might be forgotten because the agent targets them less often. Although this last point is partially mitigated by the use of learning progress for task replay, it might be a good idea to consider several multi-task, multi-goal policies when the number of task increases.

CURIOUS is an algorithm able to tackle the problem of intrinsically motivated multi-task multi-goal reinforcement learning. This problem has rarely been considered in the past, only [MACOB](https://hal.archives-ouvertes.fr/hal-01384566/document) targeted that problem and proposed a solution based on population-based and memory-based algorithms. It is a problem of importance for autonomous lifelong learning, where agents must learn and act in a realistic world with multiple tasks of different difficulties, without having access to the reward functions.

In the future, CURIOUS could be used in a hierarchical manner. A higher-level policy could feed the sequence of tasks and goals for the lower level policy to target. This would replace the current one-step policy implemented by a multi-armed bandit algorithm. 

CURIOUS is given prior information about the set of potential tasks, their associated goal space and the reward function parameterized by tasks and goals. Further work should aim at reducing the importance of these priors. Several works go in that direction and propose autonomous learning of goal representation ([Laversanne-Finot et al., 2018](https://arxiv.org/abs/1807.01521), [Nair et al., 2018](https://arxiv.org/abs/1807.04742)). Goal selection policies could also be learned online using algorithms such as [SAGG-RIAC](https://arxiv.org/abs/1301.4862) or [GoalGAN](https://arxiv.org/abs/1705.06366).

## Conclusion
This blog post presents CURIOUS, a learning algorithm that combines an extension of UVFA to enable multi-task and multi-goal RL in a single policy (E-UVFA), and active mechanisms that bias the agentâ€™s attention towards tasks where the absolute LP is maximized. With this mechanism, agents spend less time on impossible tasks and focus on achievable ones. It also helps to deal with forgetting, by refocusing learning on tasks that are being forgotten because of model faults, changes in the environment or body
changes (e.g. sensory failures). This mechanism is important for autonomous continual learning in the real world, where agents must set their own tasks and might face tasks with diverse levels of difficulty, some of which might be required to solve others later on.

## Links
* [Paper](https://arxiv.org/abs/1810.06284)
* [Code]()

## References
* [Intrinsically Motivated Goal Exploration Process](https://arxiv.org/abs/1708.02190). Forestier et al., 2017.
* [Universal Value Function Approximators](http://proceedings.mlr.press/v37/schaul15.pdf). Schaul et al., 2015.
* [Hindsight Experience Replay](https://arxiv.org/abs/1707.01495). Andrychowicz et al., 2017.
* [Unicorn: Continual Learning with a Universal, Off-policy Agent](https://arxiv.org/abs/1802.08294). Mankowitz et al., 2018.
*  [Modular Active Curiosity-Driven Discovery of Tool Use](https://hal.archives-ouvertes.fr/hal-01384566/document). Forestier et al., 2016.
* [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971). Lillicrap et al., 2015.
* [Curiosity Driven Exploration of Learned Disentangled Goal Spaces](https://arxiv.org/abs/1807.01521). Laversanne-Finot et al., 2018.
* [Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/abs/1807.04742). Nair et al., 2018.
* [Automatic Goal Generation for Reinforcement Learning Agents](https://arxiv.org/abs/1705.06366). Florensa et al., 2017.
* [Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots](https://arxiv.org/abs/1301.4862). Baranes and Oudeyer, 2013.



## Contact
Email: cedric.colas@inria.fr

-----------------
###### Subscribe to our [Twitter](https://twitter.com/@flowersINRIA).
###### Subscribe to our [RSS Feed](https://openlab-flowers.inria.fr/c/blog.rss).
###### Subscribe to our [mailing list](https://sympa.inria.fr/sympa/subscribe/flowers-blog).
